---
title: "Walmart"
author: "Lê Minh Hoàng - 22125029"
date: "2024-08-12"
output: html_document
---

# Dataset Source
https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset

# Import libraries

```{r}
library(corrplot)
library(car)
library(lmtest)
library(fastDummies)
```

```{r}
set.seed(123)
```

# 1. Load and clean data

```{r}
df = read.csv("bodyfat.csv", header = TRUE, sep=",")
head(df)
```

```{r}
df['BMI'] = 703 * df$Weight / (df$Height * df$Height)
df$BMI <- cut(df$BMI,
                       breaks = c(-Inf, 16.0, 18.4, 24.9, 29.9, 34.9, 39.9, Inf),
                       labels = c("Severely Underweight", 
                                  "Underweight", 
                                  "Normal", 
                                  "Overweight", 
                                  "Moderately Obese", 
                                  "Severely Obese", 
                                  "Morbidly Obese"),
                       right = TRUE)
summary(df)
```

```{r}
df['BMI'] = 703 * df$Weight / (df$Height * df$Height)
df$BMI = cut(df$BMI, breaks = c(-Inf, 24.9, 29.9, Inf),
                      labels = c("Underweight_and_Normal", "Overweight", "Obese"),
                      right = TRUE)

df = fastDummies::dummy_cols(df, remove_first_dummy = TRUE)
df$BMI = NULL
```

We need to delete Density since BodyFat can be calculated by Density by using Siri's (1956) equation:\
Fat (%) = (4*95/density) -4.51 x 100

```{r}
df$Density = NULL
```

```{r}
dim(df)
summary(df)
```

```{r}
sum(duplicated(df))
```

# 2. Descriptive statistics

```{r}
par(mfrow = c(3, 5))
for (col in names(df)) {
  if (is.numeric(df[[col]]) && col != 'BMI_Obese')
    hist(df[[col]], main = paste("Histogram of", col), xlab = col, breaks = 30)
}
par(mfrow = c(1, 1))
```

```{r}
par(mfrow = c(3, 5))
for (col in names(df)) {
  if (is.numeric(df[[col]]) && col != 'BMI_Obese')
    boxplot(df[col], main = col)
}
par(mfrow = c(1, 1))
```

# 3. Split data

```{r}
# Define the split ratio
train_ratio <- 0.8

# Determine the number of rows in the training set
train_size <- floor(train_ratio * nrow(df))

# Randomly sample row indices for the training set
train_indices <- sample(seq_len(nrow(df)), size = train_size)

# Split the data into training and testing sets
train_set <- df[train_indices, ]
test_set <- df[-train_indices, ]

# Display the number of rows in each set
dim(train_set) # Should be approximately 80%
dim(test_set)  # Should be approximately 20%
```

# 4. Model

## Collinearity

```{r}
pairs(subset(df))
```

```{r}
pairs(subset(df, select=c(BodyFat, Age, Chest, Hip, Wrist)))
```

```{r}
corrplot(cor(subset(df)), method = "number")
```

## Choose Model

```{r}
model_full = lm(BodyFat ~ ., data = train_set)
vif(model_full)
```

```{r}
model_full = update(model_full, . ~ . - Weight)
vif(model_full)
```

```{r}
model_full = update(model_full, . ~ . - Abdomen, data = train_set)
vif(model_full)
```

There is no VIF score greater than 10

```{r}
summary(model_full)
```

```{r}
model_reduced = update(model_full, . ~ . - Height - Neck - Thigh - Knee - Ankle - Biceps - Forearm - BMI_Obese)
anova(model_full, model_reduced)
```

```{r}
model_full = model_reduced
summary(model_full)
```

```{r}
model_bw = step(model_full, data = train_set, direction = "backward")
```

## Final Model

```{r}
model_final = model_full
summary(model_final)
```

```{r}
model_final = update(model_final, . ~ poly(Age, Chest, Hip, Wrist, degree=2))
summary(model_final)
```

```{r}
qqnorm(model_final$residuals)
qqline(model_final$residuals, col = "red")
```

```{r}
shapiro.test(model_final$residuals)
lmtest::bptest(model_final)
lmtest::dwtest(model_final, alternative="two.sided")
```

```{r}
plot(cooks.distance(model_final))
```

# 5. Evaluate the model

```{r}
predictions <- predict(model_final, newdata = test_set)
# Actual values from the test set
actual_values <- test_set$BodyFat
# Calculate Mean Squared Error (MSE) 
mse <- mean((predictions - actual_values)^2)
# Calculate R-squared 
rss <- sum((predictions - actual_values)^2) 
tss <- sum((actual_values - mean(actual_values))^2)
r_squared <- 1 - (rss / tss)
# Print metrics cat("Mean Squared Error (MSE):", mse, "\n") 
cat("R-squared:", r_squared, "\n")
```

## Validating residuals

```{r}
e <- predictions - actual_values
hist(e, main = "Histogram of Residuals", xlab = "Residuals", breaks = 10)
```

```{r}
qqnorm(e)
qqline(e, col = "red")
```

```{r}
shapiro.test(e)
```
