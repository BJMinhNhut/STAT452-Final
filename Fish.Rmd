---
title: "STAT452 Final Project - Task 2 - Fish"
author: "Luu Quoc Bao - 22125008, Le Minh Hoang - 22125029, Le Duc Nhuan - 22125070,
  Dang Minh Nhut - 22125071"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Task 2B

## 2B.1 Intial setup

### Library

```{r}
library(ggplot2)
library(car)
library(faraway)
library(corrplot)
library(dplyr)
library(tidyr)
library(lmtest)
```

### Set seed

```{r}
set.seed(452)
```

## 2B.2 Reading and preprocessing data

Data source: [kaggle](https://www.kaggle.com/datasets/vipullrathod/fish-market)

```{r}
df <- read.csv("Fish.csv", header = TRUE)
head(df)
```
### Remove missing and duplicated rows

```{r}
sum(is.na(df))
```
There is no missing values

```{r}
sum(duplicated(df))
```
There is no duplicated rows 

### Summary data

```{r}
dim(df)
```
```{r}
summary(df)
```
There are total 159 rows and 7 columns in the data. The columns are "Species", "Weight", "Length1", "Length2", "Length3", "Height", and "Width". "Species" is the only one qualitative variable, whereas  the other 6 variables are quantitative. The description of each column according to the [data source](https://www.kaggle.com/datasets/vipullrathod/fish-market):

- `Species`: This column represents the species of the fish. It is a categorical variable that categorizes each fish into one of seven species. The species may include names like "Perch," "Bream," "Roach," "Pike," "Smelt," "Parkki," and "Whitefish." This column is the target variable for the polynomial regression analysis, where we aim to predict the fish's weight based on its other attributes.
- `Weight`: This column represents the weight of the fish. It is a numerical variable that is typically measured in grams. The weight is the dependent variable we want to predict using polynomial regression.
- `Length1`: This column represents the first measurement of the fish's length. It is a numerical variable, typically measured in centimetres.
- `Length2`: This column represents the second measurement of the fish's length. It is another numerical variable, typically measured in centimetres.
- `Length3`: This column represents the third measurement of the fish's length. Similar to the previous two columns, it is a numerical variable, usually measured in centimetres.
- `Height`: This column represents the height of the fish. It is a numerical variable, typically measured in centimetres.
- `Width`: This column represents the width of the fish. Like the other numerical variables, it is also typically measured in centimetres.


### Combine length variables
Since Length1, Length2, and Length3 are the result of three different measure times, we will calculate the average of them to have the most accurate measurement.
```{r}
df$Length <- (df$Length1 + df$Length2 + df$Length3)/3
summary(df$Length)
```

Now we remove old columns of Length
```{r}
df$Length1 <- NULL
df$Length2 <- NULL 
df$Length3 <- NULL
```

### Histograms and Plots

#### Distribution of Species
```{r}
ggplot(df, aes(x = Species)) +
  geom_bar(fill = "skyblue", color = "black", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Distribution of Fish Species", x = "Species", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
We can see that Perch and Bream have more appearance in the dataset than other species.

#### Histogram of numeric columns

```{r}
df_long <- subset(df, select = -c(Species)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# Plotting histograms for all variables
ggplot(df_long, aes(x = value)) +
  geom_histogram(fill = "skyblue", color = "black", alpha = 0.7, bins=50) +
  facet_wrap(~variable, scales = "free") +
  theme_minimal() +
  labs(title = "Histograms of All Variables in df", x = "Value", y = "Frequency")
```
The histograms are not "smooth", so that seems like the distributions of length, height, and width are different among species. So we are going to research it.

#### Height by Species
```{r}
ggplot(df, aes(x = Species, y = Height)) +
  geom_boxplot(fill = "skyblue", color = "black", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Height Distribution by Species", x = "Species", y = "Height") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
The boxplot suggests that Parkki, Perch, Pike, and Roach species has relatively small different in height. On the other hand, Smelt seems to have the smallest height, and Bream has largest height.

#### Width by Species 
```{r}
ggplot(df, aes(x = Species, y = Width)) +
  geom_boxplot(fill = "skyblue", color = "black", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Width Distribution by Species", x = "Species", y = "Width") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
The boxplot suggests that Bream, Pike, and Whitefish species have the largest width. On the other hand, Smelt has the smallest width. 

#### Length by Species
```{r}
ggplot(df, aes(x = Species, y = Length)) +
  geom_boxplot(fill = "skyblue", color = "black", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Length Distribution by Species", x = "Species", y = "Length") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
The boxplot suggests that Pike species has the largest length. On the other hand, Smelt has the smallest width. 

#### Weight by Species
From 3 diagrams above, we can expect that Smelt is the smallest fish and hence has the smallest weight. And Pike, Bream might have the largest weight, since their size measurements are larger than the others.

The following boxplot confirms that.

```{r}
ggplot(df, aes(x = Species, y = Weight)) +
  geom_boxplot(fill = "skyblue", color = "black", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Weight Distribution by Species", x = "Species", y = "Weight") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
#### Boxplots and outliers detection 
```{r}
par(mfrow = c(1, 4))

for (col in names(df)) {
  if (is.numeric(df[[col]]))
    boxplot(df[col], main = col)
}

par(mfrow = c(1, 1))
```
The box plots suggest that there are not many outliers in the variables' distributions. So we will not process any outliers removal.  

## 2B.3 Data splitting

### Convert Species to factor
We convert Species to factor allowing the linear regression model to use each separate value of the column as an independent variable. Since each Species may have unique signatures that affects the Weight.
```{r}
df$Species <- as.factor(df$Species)
```

### Split

We split the data into training and testing sets. The ratio of the training set to the testing set is 80:20.

```{r}
# Define the split ratio
train_ratio <- 0.8

# Determine the number of rows in the training set
train_size <- floor(train_ratio * nrow(df))

# Randomly sample row indices for the training set
train_indices <- sample(seq_len(nrow(df)), size = train_size)

# Split the data into training and testing sets
train_set <- df[train_indices, ]
test_set <- df[-train_indices, ]

# Display the number of rows in each set
dim(train_set) # Should be approximately 80
dim(test_set)  # Should be approximately 20
```

## 2B.3 Model fitting
### Baseline model
We fit a linear regression model with all original variables to predict "Weight".

```{r}
baseline <- lm(Weight ~ ., data = train_set)
summary(baseline)
```
The T tests for significant of each variables show that there is some insignificant variables that we can remove. 

### Multicollinearity
We now check for multicollinearity to consider removing highly correlated variables.

#### Correlation matrix

```{r}
corrplot(cor(subset(df, select = -Species)), method = "number")
```
We can see that Length is highly correlated to Width (0.87), so we should consider removing one of them from the model.

#### VIF scores 
We calculate the Variance Inflation Factor (VIF) of the variables.
```{r}
vif(baseline)
```
"Height" variable has the highest VIF value and greater than 10. Therefore, we will remove it from the model.

```{r}
model1<-update(baseline, . ~ . -Height)
vif(model1)
```
"Length" variable has the highest VIF value and greater than 10. Therefore, we will remove it from the model.

```{r}
model1<-update(model1, . ~ . -Length)
vif(model1)
```

### First degree model


```{r}
model1 <- step(model, direction="backward")
summary(model1)
```
The step-wise search does not remove any variable, so we test the reduced model after using VIF scores and the full baseline model at beginning. 

```{r}
anova(model1, baseline)
```
The p-value of the ANOVA test is much higher than the significance level of 0.05, indicating that the removed variables are not significant, and we can use the reduced model.


#### Model testing
```{r}
predictions <- predict(model1, newdata = test_set) 
# Actual values from the test set 
actual_values <- test_set$Weight
# Calculate Mean Squared Error (MSE) 
mse <- mean((predictions - actual_values)^2)  
# Calculate R-squared 
rss <- sum((predictions - actual_values)^2) 
tss <- sum((actual_values - mean(actual_values))^2) 
r_squared <- 1 - (rss / tss)  
# Print metrics 
cat("Mean Squared Error (MSE):", mse, "\n") 
cat("R-squared:", r_squared, "\n")
```

#### Model diagnostics
##### Normality of residuals

We now plot the Q-Q plot of the residuals to check for normality.

```{r}
e <- model1$residuals
qqnorm(e)
qqline(e, col = "red")
```
The Q-Q plot shows that most points lie close to the line in the center, but there are deviations at both ends (tails). This suggests that while the residuals are roughly normally distributed in the middle range, there are issues in the tails. This suggests that the normality assumption may not be fully satisfied.

We perform the Shapiro-Wilk test to check for normality of the residuals.
```{r}
shapiro.test(e)
```
The p-value of the Shapiro-Wilk test is less than 0.05, indicating that the residuals are not normally distributed. This once again aligns with the Q-Q plot, where the residuals have heavier tails than a normal distribution.

##### Homoscedasticity

We also perform the Breusch-Pagan test to check for homoscedasticity.

```{r}
bptest(model)
```
The p-value of the Breusch-Pagan test is greater than 0.05, so we fail to reject the null hypothesis of homoscedasticity. We can conclude that homoscedasticity is present.

##### Autocorrelation

We perform the Durbin-Watson test to check for autocorrelation.

```{r}
dwtest(model, alternative = "two.sided")
```
The Durbin-Watson test gives a DW value close to 2, indicating that there is no autocorrelation.

### Quadratic model


```{r}
shapiro.test(model$residuals)
```

```{r}
ggplot(df, aes(x = Height, y = Width, color = Species)) +
  geom_point() +
  labs(title = "Scatter Plot of Height vs Width",
       x = "Height",
       y = "Width") +
  theme_minimal()
```




```{r}
plot(df_final$Width, df_final$Weight,
     main = "Scatter Plot of Fish Weight vs. Length1",
     xlab = "Length1",
     ylab = "Weight",
     pch = 19,       # Kiểu điểm
     col = "blue")   # Màu sắc điểm
```



```{r}
new_model = lm(Weight ~ . + I(Width^2) + I(Length1^2), data = train_set)
summary(new_model)
```

```{r}
car::vif(new_model)
```
```{r}
new_model = update(model, . ~ . - Width  + I(Width*Height*Length1), data = train_set)
summary(new_model)
```

```{r}
car::vif(new_model)
```

#### Evaluate the model
```{r}
model = new_model
# Dự đoán trên tập test với mô hình tối ưu
predictions <- predict(model, newdata = test_set)

# Tính toán các chỉ số đánh giá
mse <- mean((test_set$Weight - predictions)^2)
r_squared <- 1 - (sum((test_set$Weight - predictions)^2) / sum((test_set$Weight - mean(test_set$Weight))^2))

# In ra các chỉ số
cat("MSE:", mse, "\n")
cat("R-squared:", r_squared, "\n")

```

```{r}
predictions <- predict(model, newdata = test_set) 
# Actual values from the test set 
actual_values <- test_set$Weight
# Calculate Mean Squared Error (MSE) 
mse <- mean((predictions - actual_values)^2)  
# Calculate R-squared 
rss <- sum((predictions - actual_values)^2) 
tss <- sum((actual_values - mean(actual_values))^2) 
r_squared <- 1 - (rss / tss)  
# Print metrics cat("Mean Squared Error (MSE):", mse, "\n") 
cat("R-squared:", r_squared, "\n")
```

#### Validating residuals

```{r}
e <- predictions - actual_values
hist(e, main = "Histogram of Residuals", xlab = "Residuals", breaks = 30)
```

```{r}
qqnorm(e)
qqline(e, col = "red")
```

```{r}
shapiro.test(e)
```
```{r}
library(lmtest)
bptest(model)
```
```{r}
dwtest(model, alternative = "two.sided")
```

```{r}
shapiro.test(model$residuals)
```



```{r}
model1 = lm(Weight ~ . + poly(Height, Length1, Width, degree = 2) - Length1 - Length2 - Length3 - Height - Width, data = train_set)
summary(model1)
```
```{r}
model1 = lm(Weight ~ . -Length1 - Length2 - Length3 + I(Length1^2) + Width + I(Length1*Width) + I(Width^2) + Height + I(Height^2) + I(Height*Width), data = train_set)
summary(model1)
```

```{r}
model1 <- step(model1, data = train_set, direction = "both")
summary(model1)
```
```{r}
model1 = update(model1, . ~ . - SpeciesSmelt)
summary(model1)
```

```{r}
model1 = update(model1, . ~ . - Height)
summary(model1)
```

```{r}
car::vif(model1)
```